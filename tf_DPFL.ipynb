{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import collections\n",
    "import dp_accounting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emnist_dataset():\n",
    "  emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data(\n",
    "      only_digits=True)\n",
    "\n",
    "  def element_fn(element):\n",
    "    return collections.OrderedDict(\n",
    "        x=tf.expand_dims(element['pixels'], -1), y=element['label'])\n",
    "\n",
    "  def preprocess_train_dataset(dataset):\n",
    "    # Use buffer_size same as the maximum client dataset size,\n",
    "    # 418 for Federated EMNIST\n",
    "    return (dataset.map(element_fn)\n",
    "                   .shuffle(buffer_size=418)\n",
    "                   .repeat(1)\n",
    "                   .batch(32, drop_remainder=False))\n",
    "\n",
    "  def preprocess_test_dataset(dataset):\n",
    "    return dataset.map(element_fn).batch(128, drop_remainder=False)\n",
    "\n",
    "  emnist_train = emnist_train.preprocess(preprocess_train_dataset)\n",
    "  emnist_test = preprocess_test_dataset(\n",
    "      emnist_test.create_tf_dataset_from_all_clients())\n",
    "  return emnist_train, emnist_test\n",
    "\n",
    "train_data, test_data = get_emnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_fn():\n",
    "  model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Reshape(input_shape=(28, 28, 1), target_shape=(28 * 28,)),\n",
    "      tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "      tf.keras.layers.Dense(10)])\n",
    "  return tff.learning.models.from_keras_model(\n",
    "      keras_model=model,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      input_spec=test_data.element_spec,\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = len(train_data.client_ids)\n",
    "clients_per_thread = 5\n",
    "\n",
    "def train(rounds, noise_multiplier, clients_per_round, data_frame):\n",
    "  # Using the `dp_aggregator` here turns on differential privacy with adaptive\n",
    "  # clipping.\n",
    "  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n",
    "      noise_multiplier, clients_per_round)\n",
    "\n",
    "  # We use Poisson subsampling which gives slightly tighter privacy guarantees\n",
    "  # compared to having a fixed number of clients per round. The actual number of\n",
    "  # clients per round is stochastic with mean clients_per_round.\n",
    "  sampling_prob = clients_per_round / total_clients\n",
    "\n",
    "  # Build a federated averaging process.\n",
    "  # Typically a non-adaptive server optimizer is used because the noise in the\n",
    "  # updates can cause the second moment accumulators to become very large\n",
    "  # prematurely.\n",
    "  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
    "        my_model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n",
    "        model_aggregator=aggregation_factory)\n",
    "\n",
    "  eval_process = tff.learning.build_federated_evaluation(my_model_fn)\n",
    "\n",
    "  # Training loop.\n",
    "  state = learning_process.initialize()\n",
    "  for round in range(rounds):\n",
    "    if round % 5 == 0:\n",
    "      model_weights = learning_process.get_model_weights(state)\n",
    "      metrics = eval_process(model_weights, [test_data])['eval']\n",
    "      if round < 25 or round % 25 == 0:\n",
    "        print(f'Round {round:3d}: {metrics}')\n",
    "      data_frame = data_frame.append({'Round': round,\n",
    "                                      'NoiseMultiplier': noise_multiplier,\n",
    "                                      **metrics}, ignore_index=True)\n",
    "\n",
    "    # Sample clients for a round. Note that if your dataset is large and\n",
    "    # sampling_prob is small, it would be faster to use gap sampling.\n",
    "    x = np.random.uniform(size=total_clients)\n",
    "    sampled_clients = [\n",
    "        train_data.client_ids[i] for i in range(total_clients)\n",
    "        if x[i] < sampling_prob]\n",
    "    sampled_train_data = [\n",
    "        train_data.create_tf_dataset_for_client(client)\n",
    "        for client in sampled_clients]\n",
    "\n",
    "    # Use selected clients for update.\n",
    "    result = learning_process.next(state, sampled_train_data)\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "\n",
    "  model_weights = learning_process.get_model_weights(state)\n",
    "  metrics = eval_process(model_weights, [test_data])['eval']\n",
    "  print(f'Round {rounds:3d}: {metrics}')\n",
    "  data_frame = data_frame.append({'Round': rounds,\n",
    "                                  'NoiseMultiplier': noise_multiplier,\n",
    "                                  **metrics}, ignore_index=True)\n",
    "\n",
    "  return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame()\n",
    "rounds = 100\n",
    "clients_per_round = 50\n",
    "\n",
    "for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\n",
    "  print(f'Starting training with noise multiplier: {noise_multiplier}')\n",
    "  data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_plot(data_frame):\n",
    "  plt.figure(figsize=(15, 5))\n",
    "\n",
    "  dff = data_frame.rename(\n",
    "      columns={'sparse_categorical_accuracy': 'Accuracy', 'loss': 'Loss'})\n",
    "\n",
    "  plt.subplot(121)\n",
    "  sns.lineplot(data=dff, x='Round', y='Accuracy', hue='NoiseMultiplier', palette='dark')\n",
    "  plt.subplot(122)\n",
    "  sns.lineplot(data=dff, x='Round', y='Loss', hue='NoiseMultiplier', palette='dark')\n",
    "\n",
    "make_plot(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = 3383\n",
    "noise_to_clients_ratio = 0.01\n",
    "target_delta = 1e-5\n",
    "target_eps = 2\n",
    "\n",
    "# Initialize arguments to dp_accounting.calibrate_dp_mechanism.\n",
    "\n",
    "# No-arg callable that returns a fresh accountant.\n",
    "make_fresh_accountant = dp_accounting.rdp.RdpAccountant\n",
    "\n",
    "# Create function that takes expected clients per round and returns a \n",
    "# dp_accounting.DpEvent representing the full training process.\n",
    "def make_event_from_param(clients_per_round):\n",
    "  q = clients_per_round / total_clients\n",
    "  noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "  gaussian_event = dp_accounting.GaussianDpEvent(noise_multiplier)\n",
    "  sampled_event = dp_accounting.PoissonSampledDpEvent(q, gaussian_event)\n",
    "  composed_event = dp_accounting.SelfComposedDpEvent(sampled_event, rounds)\n",
    "  return composed_event\n",
    "\n",
    "# Create object representing the search range [1, 3383].\n",
    "bracket_interval = dp_accounting.ExplicitBracketInterval(1, total_clients)\n",
    "\n",
    "for target_eps in [2.0, 4.0, 6.0, 8.0]:\n",
    "  # Perform search for smallest clients_per_round achieving the target privacy.\n",
    "  clients_per_round = dp_accounting.calibrate_dp_mechanism(\n",
    "      make_fresh_accountant, make_event_from_param, target_eps, target_delta,\n",
    "      bracket_interval, discrete=True\n",
    "  )\n",
    "\n",
    "  noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "  print(f'To get ({target_eps}, {target_delta})-DP, use {clients_per_round} '\n",
    "        f'clients with noise multiplier {noise_multiplier}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 100\n",
    "noise_multiplier = 1.2\n",
    "clients_per_round = 120\n",
    "\n",
    "data_frame = pd.DataFrame()\n",
    "data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
    "\n",
    "make_plot(data_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
